\chapter{Extensions}

\section{Okapi BM25}

\subsection{Basic Approach}
The Okapi BM25 algorithm is a more sophisticated type of $TFIDF$ ranking algorithm.
It's a summation over all words that make up the search term, making use of the $TF$ calculation as well as the $IDF$ calculation along with variables that can be used for optimisation. \citep{robertson2009probabilistic}
The version of the formula used in this project is:
\begin{align}
    OBM25 &= \sum_{i=1}^n IDF(w_i) \cdot \frac{TF(w_i)\cdot (k_1 + 1)}{TF(w_i) + k_1\cdot \big( 1 - b + b\cdot \frac{W}{W_a} \big)}
    \label{eq:OBM25}
\end{align}

with the optimisation variables set as $k_1 = 1,2$ and $b = 0,75$ since no advanced optimisation was considered and

\begin{itemize}
    \item $OMB25$ is the Okapi BM 25 score
    \item A search term $w$ consists of individual words $w_1, w_2, ..., w_n$
    \item $IDF(w_i)$ is the inverse document frequency score applied to the word $w_i$
    \item $TF(w_i)$ is the term frequency score applied to the word $w_i$
    \item $W$ is the number of words on the website
    \item $W_a$ is the average number of words on a website
\end{itemize}

\subsection{Technical Description}
As the Okapi BM25 algorithm makes use of both the $TF$ and the $TFIDF$ calculations, these objects were stored as fields in the {\tt OkapiBM25Score} class in a similar manner to what was done for the {\tt TFIDFScore} class. The optimisation constants and the average document length were set as static fields. Two helper methods were added:
\begin{itemize}
    \item {\tt setAverageDocLength} calculates the mean number of words per website based on the websites in the index
    \item {\tt okapiScore} is a recursive method to perform the summation of all the individual scores of all the words in the search query to return to the {\tt getScore} method
\end{itemize}

\subsection{Testing Considerations}
The mathematical correctness of the OBM25 score calculations were verified using unit tests, which can be found in the ScoreTest.java file alongside the unit tests for the other {\tt Score} classes.
The set up was the same as previously mentioned in the Ranking Algorithm's chapter.
Positive tests for single word and multi word query tests were constructed in the following manner:
\begin{itemize}
    \item the word doesn't occur on the specified website
    \item the word doesn't occur in the website index at all
    \item the word occurs once on the specified website
    \item the word occurs once on the specified website and at least one other website
    \item the word occurs more than once on the specified website
    \item multi-word query: words don't occur on the specified website
    \item multi-word query: words occur once on the specified website
    \item multi-word query: words occur more than once on the specified website
    \item multi-word query: words occur once on the specified website and at least one other website
    \item comparison of the above score values
\end{itemize}

Negative testing consideration were harder to formulate.
The most obvious to test would be dividing by 0, however as seen in equation \ref{eq:OBM25}, it's not actually possible for the denominator to be 0 due to the optimisation variables: either $TF(w_i)$ or $\frac{W}{W_a}$ has to be negative, which is not possible.
To that end, no negative tests were considered for the {\tt OkapiBM25} class.

\section{Improve the Client GUI} % Change the client code such that the result of searches are displayed in a nicer way.

We were given the possibility to improve the front-end of the search engine as an added task. The client side of our product consists of a set of files that dictate, among other things, the aspect of the page, implements the pieces and bits of code that will ultimately allow the user to communicate with the server and perform the searches, and arranges the results of the queries in a more user-friendly fashion.\\
Wireframes of graphical user interface designs can be found in the appendix. The guidelines defined by such wireframes took in consideration the "10 Timeless Commandments for Good Design", by the German industrial designer Dieter Rams. \citep{domingo:designprinciples}
\begin{itemize}
    \item The search bar was brought to the centre of user's vision and any other information that was not related to the basic functionality of searching of websites was made smaller and away from that area:
    \begin{itemize}
        \item "Good design makes a product useful. A product is bought to be used. It has to satisfy certain criteria, not only functional, but also psychological and aesthetic. Good design emphasises the usefulness of a product whilst disregarding anything that could possibly detract from it." \citep{domingo:designprinciples}
        \item "Good design makes a product understandable. It clarifies the product’s structure. Better still, it can make the product talk. At best, it is self-explanatory." \citep{domingo:designprinciples}
        \item "Good design is unobtrusive. Products fulfilling a purpose are like tools. They are neither decorative objects nor works of art. Their design should therefore be both neutral and restrained, to leave room for the user’s self-expression." \citep{domingo:designprinciples}
        \item "Good design is honest. It does not make a product more innovative, powerful or valuable than it really is. It does not attempt to manipulate the consumer with promises that cannot be kept." \citep{domingo:designprinciples}
        \item "Good design is honest. It does not make a product more innovative, powerful or valuable than it really is. It does not attempt to manipulate the consumer with promises that cannot be kept." \citep{domingo:designprinciples}
        \item "Good design is as little design as possible. Less, but better – because it concentrates on the essential aspects, and the products are not burdened with non-essentials. Back to purity, back to simplicity." \citep{domingo:designprinciples}
    \end{itemize}
    \item The colour scheme as well as the background were carefully selected to pass the impression to the user that they are searching through a big number of possibilities. What is the entity that is big and contain all of the beautiful things? — the Universe, of course.
    \begin{itemize}
        \item "Good design is aesthetic. The aesthetic quality of a product is integral to its usefulness because products we use every day affect our person and our well-being. But only well-executed objects can be beautiful." \citep{domingo:designprinciples}
    \end{itemize}
    \item The aspect of the graphical user interface adjusts to the size of the viewport, making the website responsive:
    \begin{itemize}
        \item "Good design is thorough down to the last detail. Nothing must be arbitrary or left to chance. Care and accuracy in the design process show respect towards the user." \citep{domingo:designprinciples}
    \end{itemize}
\end{itemize}

The files containing the code that concerns the front-end of the search engine can be found in the folder static. Here follows each of the files' description:
\begin{itemize}
    \item index.html — This is the first file the browser reads upon accessing the root of a website hosted in any given domain. Hence it refers to what other files to read as well (such as the styling sheet). It provides written unformatted text which will be displayed on the browser, and it can also include links to other webpages.
    \item style.css — It is possible to style a given webpage from a given html file, but it is best practice to do in on a separate file (such as the present one). Should one build a website with several pages (which for each a separate html file is necessary), styling can become cumbersome and even result in styling inconsistencies. So this file provides a styling guide that can be used for several different pages providing consistency among all of them, and for this is only necessary to add the line of code that points to such file in the html. As it becomes obvious, in the long this practice also saves time as no more styling is required as the website grows.
    \item code.js — It holds the javascript code that allows for changes in the html (or even style), which will result in changes on what the user sees. Our javascript code was responsible for receiving the search term(s), sending them to the server, receiving the results of the given search, and translating them into html.
    \item Image files in static/img/ — Some images needed to provide the website with the desired aspect.
\end{itemize}
The basic implementation of the client GUI allowed the very basic functionality of performing a search. So the accomplished tasks in this regard will be described in the following subsections.

\subsection{Adding content to be displayed by the html}
Several aspects of the GUI were changed to improve the user's experience. We included a footer with links to ITU's website, the course page, as well as our LinkedIn profiles. Overall names of the classes and id's to be used in the styling sheet were also changed to achieve the intended design. Code was also added to include background images.
Additionally, it seemed intuitive to allow the enter key to trigger a search, so such feature was implemented by including a small script in the html file.

\subsection{Styling}
All the aspect of the website was described in the style.css file. In here, virtually everything was changed, namely:
\begin{itemize}
    \item Centering the content of the webpage;
    \item The aspect of every given class, id, link, as well as behaviour of certain elements when, for example, the user hovers the mouse over that specific element;
    \item Providing responsiveness no the website (adjusting the aspect of the content depending on the size of the viewport;
    \item Behaviour of the background images, where they would display as crossfaded slide show;
    \item Behaviour of the searchbar, where it would change its size by clicking on it.
\end{itemize}

\subsection{Adding functionality through javascript}
Changes in the javascript code, which can be found in the static/code.js, where made in order to allow for:
\begin{itemize}
    \item Provide a different answer depending on the given different queries. The cases we accounted for were the following:
    \begin{itemize}
        \item No query was provided;
        \item The query did not provide any result;
        \item The query in a number of results different than $0$.
    \end{itemize}
    \item Besides the title and the URL, the results are also accompanied by a certain number word from the given website.
    \item Clicking on a result will open it on a new tab (instead of the current tab).
\end{itemize}

\section{Auto-complete}
\subsection{Task}
Auto-complete is a feature that suggests possible input based on the current input as one types on a drop-down list. One of the extensions was implementing this feature.

\subsection{Basic Approach}
The JavaScript sits on the client side of the search engine in the static folder in a file named {\tt code.js}. It is responsible for the behaviour of the webpage and is one of 3 files that inform how the webpage looks and acts (the other two being {\tt index.html} and {\tt style.css}).
As such, it is also responsible for communicating with the server to send and retrieve data. The JavaScript communicates with the sever using JSON. The server then parses the JSON request and sends a response back to client for it to then handle.\\
The JQuery UI offers an auto-complete plugin so little JavaScript is required to implement the feature. All that is required is to link the plugin to the input field in your HTML and then define a data source for your auto-complete feature to draw its suggestions from. The simplest way to accomplish that is to define a local array of words in your JavaScript file, but this method only works for small data sizes and therefore is not suitable for implementation in a search engine. Which means that the alternative is to retrieve data from the server and use that as the source for the plugin.

\subsection{Technical Description}
Firstly, the JQuery UI plugin was implemented in the {\tt code.js} file and linked to the input field {\tt searchbox}. Rather than sourcing a local array, a {\tt function(request, response)} was created as the source which makes a call to the server for data and performs some action on the response.\\
Originally, the idea was that each character typed in the input field would trigger a call to server and a method in the {\tt WebApplication} class which would check a list of words (built from the Index) to see whether any words contained the request term and build a list of the matches to return to the client. However, this idea would result on many calls being made to the server, which would slow down the operation of the search engine substantially. This was then revised to make a call to the server once and store the response locally as the auto-complete plugin source, and the method to return the data was revised in turn.\\
Next, the number of results was trimmed so that the drop-down list of suggestions didn't stretch the entire length of the page (as users should not be required to scrolled through results), and adjusted the plugin options so that results are only returned after at least 3 characters are in the input field.\\
Finally, the results returned matching the request term were ordered so that results beginning with the request term appeared at the top of the drop-down list of suggestions.\\
The styling for the plugin had to be added so as to match the styling of the rest of the webpage (colour, background etc), as well responsiveness to page resizing.

\section{WebCrawler}
\subsection{Basic Approach}

In this section the flow of the WebCrawler will be explained.
WebCrawler solutions contains three classes {\tt WebCrawler}, {\tt WebScraper} and {\tt DocumentHelper}.

\begin{itemize}
    \item The {\tt WebCrawler} class contains the {\tt main} method which will instantiate a {\tt WebCrawler} object and start the whole crawling process. It does so by having a list of default website urls which it will use as starting point for traversing the internet. For extracting information and crawling from website to website it uses the the recursive method {\tt fetchWebsiteRecursive} in the {\tt WebScraper} class.;
    \item For each call to the method {\tt fetchWebsiteRecursive} , the {\tt WebScraper} will download the html page and extract or scrape its information and store it in an index file.;
    \item From each website the {\tt WebScraper} will also extract all the links, and for all the links pointing to websites that it have not retrieved previously, it will recursively call the {\tt fetchWebsiteRecursive} method giving in these new link as input. For doing the actual extraction of  information, the {\tt WebScraper} make use of the helper class {\tt DocumentHelper} class;
    \item {\tt DocumentHelper} contains helper methods that can extract information from a given url address. It does so by utilizing a java library {\tt org.jsoup.Jsoup} LINK. The Jsoup library will download a html document and parse it into a data structure which can be queried for specific html tags.
\end{itemize}

\subsection{Technical description}
This section takes a closer look at the technical solution of the Web Crawler and fallows the flow of the programm.
\begin{itemize}
    \item As mentioned the {\tt WebCrawler} class contains a {\tt main} method which instantiate an object of the {\tt WebCrawler} itself. In its constructer it gets the {\tt ArrayList<Sting>} of default urls to look at by calling the method {\tt getUrlsToLookAt()}. After the object has been created the main method calls the method {\tt crawl()}.
    The {\tt crawl()} method first deletes previous Web Crawler results with  {\tt deleteFileIfExist()} method. Next it loops through the list of {\tt defaultUrls} and for each of them it will get the components of the url by creating a {\tt URL} object from it. From the components it will concatenate {\tt Host} and {\tt Path} into the {\tt String urlValue} and adds this value to the {\tt HashSet<String> visitedSites}. This HashSet is used for maintaining an overview of which websites have already been visited, thereby making sure the whole crawling process don’t end up in an infinite loop of calling the same websites repeatedly. The {\tt HashSet<String> visitedSites} and the {\tt String urlValue} is now passed to the {\tt fetchWebsiteRecursive} method in {\tt WebScraper} class which starts the recursive crawling of the internet.
    \item {\tt fetchWebsiteRecursive} method in {\tt WebScraper} takes in the parameters {\tt String url} and a {\tt HashSet visitedSites}. In this method the {\tt org.jsoup.Jsoup} solution is used to  download the html document from the given urlValue and store it in a {\tt Document} object from the same library. This object is then parsed to the {\tt extractWebsiteFromDoc} method in the {\tt DocumentHelper} class.\\
    In here the method of {\tt Element.getElementsByTag(String tagName)} found in the {\tt org.jsoup.jsoup}\footnotemark library, is used to fetch the different html tags containing the values that is needed for the index file, such as the <title> and <h1-3> tags. For example for getting the title, one first get the Element object representing the <head> tag, and from that Element query for the Element representing the <title> tag. When it is obtained it is possible to get its text by calling the {\tt Element.text()}.
    If the passed {\tt Document} has valid {\tt title}, the method will go on to extract the words the same way as the title. Here it is looking for all the <h1-3> tags.
    Theese specific tags were chosen after briefly analyzing several webpages.It became apparent that content of the websites is stored in the headers of the html documents, and for this initial version of the Web Crawler it has been found sufficient to start by using only a few headers. These header objects are then stored in the {\tt ArrayList<Element> allHeaderElements}.\\
    The method continues by selecting the words to store in the {\tt ArrayList<String> words} by looping through the {\tt ArrayList<Element> allHeaderElements}. If the {\tt header} tag contains text and has child nodes, then another {\tt forEach} loop is having a look at those {\tt childNodes} and take only those that are {\tt TextNode}s, leaving out links and deeper level elements.\\
    Each text in the header is then split by space in order to get the individual words. Each word is then added to the {\tt ArrayList<String> words} after it has been sent to {\tt cleanWord} method where punctuation are removed.  This process continues till all the headers have been checked and all the words added to the {\tt words collection}.  At the end it then uses the title and words collection to create a new object of type {\tt Website} which is returned back to the {\tt WebScraper}.
    \footnotetext{jsoup \url{https://jsoup.org} jsoup is a Java library for working with real-world HTML. It provides a convenient API for extracting and manipulating data.}
    \item The {\tt Website} object created by the {\tt extractWebsiteFromDoc} method is passed over to the {\tt AppendSiteToFile} method in {\tt WebScraper} class. {\tt Website} is appended to the file {\tt data\_real\_data\_file.txt} in a format that integrates this data set to fit with the rest of the program.
    \item After the {\tt AppendSiteToFile} method has finished the {\tt WebScraper} class continue with extracting links from the current html document. It does so by again using a method in the {\tt DocumentHelper} class called {\tt extractLinksFromDoc}. This method returns an Elements collection which contains all the link Element objects. The {\tt WebScraper} then loops over these links, and if a link is valid and not already in the {\tt HashSet<String> visitedSites}, recursively parse it to the {\tt fetchWebsiteRecursive} method and also add it in the {\tt visitedSites}, thereby traversing to the next website and execute the whole process again. If the recursive call finish, it will go all the way back to the {\tt WebCrawler} class and fetch the next default url and recursively start from there.
\end{itemize}

\subsection{Testing}
To ensure that the functionality of the {\tt WebCrawler} is doing as intended, basic unit tests have been created which test that the correct information is extracted from a html document. For this purpose a test html document with a title, some links and few words has been setup in the unit test class {\tt WebCrawlerTest}. This unit test class contains the following test methods:
\begin{itemize}
    \item {\tt correctTitle()};
    \item {\tt correctNumberOfWords()};
    \item {\tt correctUrl()};
    \item {\tt correctNumberOfLinks()}.
\end{itemize}

The unit test {\tt correctNumberOfWords } reviled that the initial functionality of the {\tt WebCrawler} was not handling the <h1-3> tags correctly if they contained other tags besides the text. This led to changes in the code, so it now loops through the child tags of a <h1-3> tag, and only extracting words from child if it is in a text format. Using the {\tt org.jsoup.Jsoup} library, it means that only words from a child node of type TextNode are extracted.

\subsection{Reflection and considerations}
There are several adjustments that became apparent after trying to run the WebCrawler. Example, program could be optimized in a way that WebCrawler would disregard links that lead to information that do not distinguish and is not particularly interesting, like contact, login or links to webpages that contains information about the companies that provide technical support to the webpage erc.\\
Large web crawlers like used by Google, are more likely to be interested to reach out to everything available on the internet, however one developing their own web crawler would consider to limit the search due to limited resources, like memory or time. The Web Crawler is retrieving websites recursively and first goes as far as it can from the initial website and starts to return back once it is not possible to go any further (there are no more links on the website reached). In the dataset that was created over running Web Crawler for little under six hours and once it ‘left’ the first default url homepage, it never returned back to fetch the second default link.\\
There are different ways to optimize the Web Crawler, depending on the use case. One cold be interested in a specific domains, like hotels, airline information and could restrict web crawler to search information only in the domains of interest by not letting it traverse away from the default host domains.
If one would be interested to crawl specific websites, the html documents of these websites could be analyzed, to understand where the information one is looking for is located, instead of the current setting, where the WebCrawler is only looking for information located in <h1-3> tags.

%How to reference surce\footnotemark.